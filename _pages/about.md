---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently an associate professor in [John Hopcroft Center](https://jhc.sjtu.edu.cn/) of Shanghai Jiao Tong University.

My research interests include AR/VR, avatars/characters, 3D animations, HCI, and computer graphics. Previously, I was an Associate Research Scientist in AR/VR at Disney Research Los Angeles. I received the B.Sc. degree in communication and information engineering from Purdue/UESTC in 2010 and the Ph.D. degree in computer graphics from the University College London (UCL) in 2015. I has served as Associate Editor of the International Journal of Human Computer Studies, and a regular member of IEEE virtual reality program committees.

I am looking for self-motivated PhD, master and undergraduate students to join my research group. My E-mail address is **whitneypanye@sjtu.edu.cn**. Please contact me if you are interested.


# üî• News
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìù Publications 


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">US Patent 2021</div><img src='images/Gaze_based_rendering_for_audience_engagement.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Gaze based rendering for audience engagement](https://patents.google.com/patent/US11189047B2/en)

**Ye Pan**, Kenny Mitchell

- The present disclosure is related to an audience engagement system and method to display images on a display. The method includes detecting a gaze direction of a designated viewer, rendering a gaze object within an image on a gaze axis corresponding to the gaze direction, rendering an audience object within the image on a normal axis corresponding to a display axis, composting the audience object and the gaze object together in a composited image, and displaying the composited image on the display.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CG 2021</div><img src='images/Foreward_to_the_special_section_on_the_Reality-Virtuality_Continuum_and_its_Applications_(RVCA).png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Foreward to the Special Section on the Reality-Virtuality Continuum and its Applications (RVCA)](https://napier-repository.worktribe.com/output/2776183)

Mashhuda Glencross, Kenny Mitchell, Billinghurst Mark, **Ye Pan**

- Purpose: To survey soccer practitioners‚Äô recovery strategy:(1) use,(2) perceived effectiveness, and (3) factors influencing their implementation in professional soccer. Methods: A cross-sectional convenience sample of professional soccer club/confe‚Ä¶ Read More about The Use of Recovery Strategies in Professional Soccer: A Worldwide Survey.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Frontiers in virtual reality 2020</div><img src='images/The_rocketbox_library_and_the_utility_of_freely_available_rigged_avatars.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[The rocketbox library and the utility of freely available rigged avatars](https://www.frontiersin.org/journals/virtual-reality/articles/10.3389/frvir.2020.561558/full)

Mar Gonzalez-Franco, Eyal Ofek, **Ye Pan**, Angus Antley, Anthony Steed, Bernhard Spanlang, Antonella Maselli, Domna Banakou, Nuria Pelechano, Sergio Orts-Escolano, Veronica Orvalho, Laura Trutoiu, Markus Wojcik, Maria V Sanchez-Vives, Jeremy Bailenson, Mel Slater, Jaron Lanier

- As part of the open sourcing of the Microsoft Rocketbox avatar library for research and academic purposes, here we discuss the importance of rigged avatars for the Virtual and Augmented Reality (VR, AR) research community. Avatars, virtual representations of humans, are widely used in VR applications. Furthermore many research areas ranging from crowd simulation to neuroscience, psychology, or sociology have used avatars to investigate new theories or to demonstrate how they influence human performance and interactions. We divide this paper in two main parts: the first one gives an overview of the different methods available to create and animate avatars. We cover the current main alternatives for face and body animation as well introduce upcoming capture methods. The second part presents the scientific evidence of the utility of using rigged avatars for embodiment but also for applications such as crowd simulation and entertainment. All in all this paper attempts to convey why rigged avatars will be key to the future of VR and its wide adoption.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">VR 2020</div><img src='images/PoseMMR_A_Collaborative_Mixed_Reality_Authoring_Tool_for_Character_Animation.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PoseMMR: A Collaborative Mixed Reality Authoring Tool for Character Animation](https://ieeexplore.ieee.org/abstract/document/9090677)

**Ye Pan**, Kenny Mitchell

- Augmented reality devices enable new approaches for character animation, e.g., given that character posing is three dimensional in nature it follows that interfaces with higher degrees-of-freedom (DoF) should outperform 2D interfaces. We present PoseMMR, allowing Multiple users to animate characters in a Mixed Reality environment, like how a stop-motion animator would manipulate a physical puppet, frame-by-frame, to create the scene. We explore the potential advantages of the PoseMMR can facilitate immersive posing, animation editing, version control and collaboration, and provide a set of guidelines to foster the development of immersive technologies as tools for collaborative authoring of character animation.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP 2020</div><img src='images/Pose-Guided_Person_Image_Synthesis_in_the_Non-Iconic_Views.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Pose-guided person image synthesis in the non-iconic views](https://ieeexplore.ieee.org/document/9200521)

Chengming Xu, Yanwei Fu, Chao Wen, **Ye Pan**, Yu-Gang Jiang, Xiangyang Xue

- Generating realistic images with the guidance of reference images and human poses is challenging. Despite the success of previous works on synthesizing person images in the iconic views, no efforts are made towards the task of pose-guided image synthesis in the non-iconic views. Particularly, we find that previous models cannot handle such a complex task, where the person images are captured in the non-iconic views by commercially-available digital cameras. To this end, we propose a new framework - Multi-branch Refinement Network (MR-Net), which utilizes several visual cues, including target person poses, foreground person body and scene images parsed. Furthermore, a novel Region of Interest (RoI) perceptual loss is proposed to optimize the MR-Net. Extensive experiments on two non-iconic datasets, Penn Action and BBC-Pose, as well as an iconic dataset - Market-1501, show the efficacy of the proposed model that can tackle the problem of pose-guided person image generation from the non-iconic views. The data, models, and codes are downloadable from [https://github.com/loadder/MR-Net](https://github.com/loadder/MR-Net).
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">VRW 2020</div><img src='images/Group-Based_Expert_Walkthroughs_How_Immersive_Technologies_Can_Facilitate_the_Collaborative_Authoring_of_Character_Animation.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Group-based expert walkthroughs: How immersive technologies can facilitate the collaborative authoring of character animation](https://ieeexplore.ieee.org/document/9090619)

**Ye Pan**, Kenny Mitchell

- Immersive technologies have increasingly attracted the attention of the computer animation community in search of more intuitive and effective alternatives to the current sophisticated 2D interfaces. The higher affordances offered by 3D interaction, as well as the enhanced spatial understanding have the potential to improve the animators‚Äô task, which is tremendously skill intensive and time-consuming. We explore the capabilities provided by our PoseMMR, multiple users posing and animating characters in a mixed reality (MR) environment, animation via group-based expert walkthroughs. We demonstrated our system can facilitate immersive posing, animation editing, version control and collaboration. We provide a set of guidelines and discussed the benefits and potential of immersive technologies for our future animation toolsets.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJHCS 2020</div><img src='images/Improving_VIP_viewer_gaze_estimation_and_engagement_using_adaptive_dynamic_anamorphosis.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Improving VIP viewer gaze estimation and engagement using adaptive dynamic anamorphosis](https://www.sciencedirect.com/science/article/abs/pii/S1071581920301658)

**Ye Pan**, Kenny Mitchell

- Anamorphosis for 2D displays can provide viewer centric perspective viewing, enabling 3D appearance, eye contact and engagement, by adapting dynamically in real time to a single moving viewer‚Äôs viewpoint, but at the cost of distorted viewing for other viewers. We present a method for constructing non-linear projections as a combination of anamorphic rendering of selective objects whilst reverting to normal perspective rendering of the rest of the scene. Our study defines a scene consisting of five characters, with one of these characters selectively rendered in anamorphic perspective. We conducted an evaluation experiment and demonstrate that the tracked viewer centric imagery for the selected character results in an improved gaze and engagement estimation. Critically, this is performed without sacrificing the other viewers‚Äô viewing experience. In addition, we present findings on the perception of gaze direction for regularly viewed characters located off-center to the origin, where perceived gaze shifts from being aligned to misalignment increasingly as the distance between viewer and character increases. Finally, we discuss different viewpoints and the spatial relationship between objects.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Frontiers in Robotics and AI 2019</div><img src='images/How_foot_tracking_matters_The_impact_of_an_animated_self-avatar_on_interaction_embodiment_and_presence_in_shared_virtual_environments.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[How foot tracking matters: The impact of an animated self-avatar on interaction, embodiment and presence in shared virtual environments](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2019.00104/full)

**Ye Pan**, Anthony Steed

- The use of a self-avatar representation in head-mounted displays has been shown to have important effects on user behavior. However, relatively few studies focus on feet and legs. We implemented a shared virtual reality for consumer virtual reality systems where each user could be represented by a gender-matched self-avatar controlled by multiple trackers. The self-avatar allowed users to see their feet, legs and part of their torso when they looked down. We implemented an experiment where participants worked together to solve jigsaw puzzles. Participants experienced either no-avatar, a self-avatar with floating feet, or a self-avatar with tracked feet, in a between-subjects manipulation. First, we found that participants could solve the puzzle more quickly with self-avatars than without self-avatars; but there was no significant difference between the latter two conditions, solely on task completion time. Second, we found participants with tracked feet placed their feet statistically significantly closer to obstacles than participants with floating feet, whereas participants who did not have a self-avatar usually ignored obstacles. Our post-experience questionnaire results confirmed that the use of a self-avatar has important effects on presence and interaction. Together the results show that although the impact of animated legs might be subtle, it does change how users behave around obstacles. This could have important implications for the design of virtual spaces for applications such as training or behavioral analysis.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">VRST 2019</div><img src='images/Avatar_type_affects_performance_of_cognitive_tasks_in_virtual_reality.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Avatar type affects performance of cognitive tasks in virtual reality](https://dl.acm.org/doi/abs/10.1145/3359996.3364270)

**Ye Pan**, Anthony Steed

- Current consumer virtual reality applications typically represent the user by an avatar comprising a simple head/torso and decoupled hands. In the prior work of Steed et al. it was shown that the presence or absence of an avatar could have a significant impact on the cognitive load of the user. We extend that work in two ways. First they only used a full-body avatar with articulated arms, so we add a condition with hands-only representation similar to the majority of current consumer applications. Second we provide a real-world benchmark so as to start to get at the impact of using any immersive system. We validate the prior results: real and full body avatar performance on a memory task is significantly better than no avatar. However the hands only condition is not significantly different than either these two extremes. We discuss why this might be, in particular we discuss the potential for a individual variation in response to the embodiment level.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ISMAR 2019</div><img src='images/Accurate_and_fast_classification_of_foot_gestures_for_virtual_locomotion.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Accurate and fast classification of foot gestures for virtual locomotion](https://ieeexplore.ieee.org/abstract/document/8943618)

Xinyu Shi, Junjun Pan, Zeyong Hu, Juncong Lin, Shihui Guo, Minghong Liao, **Ye Pan**, Ligang Liu

- This work explores the use of foot gestures for locomotion in virtual environments. Foot gestures are represented as the distribution of plantar pressure and detected by three sparsely-located sensors on each insole. The Long Short-Term Memory model is chosen as the classifier to recognize the performer's foot gesture based on the captured signals of pressure information. The trained classifier directly takes the noisy and sparse input of sensor data, and handles seven categories of foot gestures (stand, walk forward/backward, run, jump, slide left and right) without manual definition of signal features for classifying these gestures. This classifier is capable of recognizing the foot gestures, even with the existence of large sensor-specific, inter-person and intra-person variations. Results show that an accuracy of ~80% can be achieved across different users with different shoe sizes and ~85% for users with the same shoe size. A novel method, Dual-Check Till Consensus, is proposed to reduce the latency of gesture recognition from 2 seconds to 0.5 seconds and increase the accuracy to over 97%. This method offers a promising solution to achieve lower latency and higher accuracy at a minor cost of computation workload. The characteristics of high accuracy and fast classification of our method could lead to wider applications of using foot patterns for human-computer interaction.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Mechanism and Machine Theory 2019</div><img src='images/Enabling_grasp_action_generalized_quality_evaluation_of_grasp_stability_via_contact_stiffness_from_contact_mechanics_insight.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Enabling grasp action: Generalized quality evaluation of grasp stability via contact stiffness from contact mechanics insight](https://www.sciencedirect.com/science/article/abs/pii/S0094114X18318639)

Huixu Dong, Chen Qiu, Dilip K Prasad, **Ye Pan**, Jiansheng Dai, I-Ming Chen

- Performing a grasp is a pivotal capability for a robotic gripper. We propose a new evaluation approach of the quality of grasping stability via constructing a model of grasping stiffness based on the theory of contact mechanics. First, the mathematical models are built to explore ‚Äúsoft contact‚Äù and the general grasp stiffness between a finger and an object. Next, the grasping stiffness matrix is constructed to reflect the normal, tangential and torsion stiffness coefficients. Finally, we design two grasping cases to verify the proposed measurement criterion of the quality of grasping stability by comparing different grasping configurations. Specifically, a standard grasping index is used and compared with the minimum eigenvalue index of the constructed grasping stiffness we built. The comparison result reveals a similar tendency between them for measuring the quality of grasping stability and thus, validates the proposed approach.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICIDS 2019</div><img src='images/JUNGLE_an_interactive_visual_platform_for_collaborative_creation_and_consumption_of_nonlinear_transmedia_stories.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[JUNGLE: an interactive visual platform for collaborative creation and consumption of nonlinear transmedia stories](https://link.springer.com/chapter/10.1007/978-3-030-33894-7_26)

Mubasir Kapadia, Carlos Manuel Muniz, Samuel S Sohn, **Ye Pan**, Sasha Schriber, Kenny Mitchell, Markus Gross

- JUNGLE is an interactive, visual platform for the collaborative manipulation and consumption of nonlinear transmedia stories. Intuitive visual interfaces encourage JUNGLE users to explore vast libraries of story worlds, expand existing stories, or conceive of entirely original story worlds. JUNGLE stories utilize multiple media forms including videos, images, and text, and accommodate branching narrative outcomes. We extensively evaluate Jungle using a focused small-scale study and free-form large-scale study with careful protection of study participant privacy. In the small-scale study, users found JUNGLE ‚Äôs features to be versatile, engaging, and intuitive for discovering new content. In the large-scale study, 354 subjects tested JUNGLE in a realistic 45-day scenario. We find that users collaborated on story worlds incorporating various forms of media in multiple (on average two) possible story paths. In particular, we find through initial observations that JUNGLE can evoke creativity: traditionally passive consumers gradually transition into active content creators. Supplementary videos showcasing the JUNGLE system and hypothetical example stories authored using JUNGLE independently hosted [here](https://www.youtube.com/watch?v=D_aHkhQJgQQ) and [here](https://www.youtube.com/watch?v=CL4g8YDM0tE).
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Frontiers in Robotics and AI 2018</div><img src='images/We_Wait_the_impact_of_character_responsiveness_and_self_embodiment_on_presence_and_interest_in_an_immersive_news_experience.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[‚ÄúWe wait‚Äù‚Äîthe impact of character responsiveness and self embodiment on presence and interest in an immersive news experience](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2018.00112/full)

Anthony Steed, **Ye Pan**, Zillah Watson, Mel Slater

- A virtual reality scenario called ‚ÄúWe Wait‚Äù gives people an immersive experience of the plight of refugees waiting to be picked up by a boat on a shore in Turkey to be illegally taken to Europe, crossing a dangerous stretch of sea. This was based on BBC news reporting of the refugee situation, but deliberately depicted as an animation with cartoon-like characters representing the refugees. Of interest was the level of presence that might be experienced by participants and the extent to which the scenario might prompt participants to follow-up further information about the refugee crisis. By presence we refer to both Place Illusion, the illusion of being in the rendered space, and Plausibility, the illusion that the unfolding events were really happening. The follow-up was assessed by whether and when participants accessed a web page that contained further information about the refugee crisis after the experiment. Two factors were considered in a balanced between-groups design with 32 participants. The Responsiveness factor was either ‚ÄúNone‚Äù or ‚ÄúLook at.‚Äù In the first the virtual characters in the scenario never responded to actions of the participant, and in the second they would occasionally look at the participant after the participant looked at them. The second factor was Embodiment, which was either ‚ÄúNo Body‚Äù or ‚ÄúBody.‚Äù In the No Body condition participants had no virtual body, and in the Body condition they would see a virtual body spatially congruent with their own if they looked down toward themselves. The virtual body was animated by the head tracking move the upper body. The results showed that the major factor positively contributing to presence was Responsiveness (‚ÄúLook at‚Äù), and that Embodiment (‚ÄúBody‚Äù) may have contributed but to a lesser extent. There were important differences between men and woman in the degree of follow-up, with men more likely to do so than women. The experiment shows that adding in some simple responses in an immersive journalism scenario, where the characters acknowledge the presence of the participant through gaze, can enhance the degree of presence felt by the participants.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CAVW 2018</div><img src='images/Empowerment_and_embodiment_for_collaborative_mixed_reality_systems.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Empowerment and embodiment for collaborative mixed reality systems](https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.1838)

**Ye Pan**, David Sinclair, Kenny Mitchell

- We present several mixed‚Äêreality‚Äêbased remote collaboration settings by using consumer head‚Äêmounted displays. We investigated how two people are able to work together in these settings. We found that the person in the AR system will be regarded as the ‚Äúleader‚Äù (i.e., they provide a greater contribution to the collaboration), whereas no similar ‚Äúleader‚Äù emerges in augmented reality (AR)‚Äêto‚ÄêAR and AR‚Äêto‚ÄêVRBody settings. We also found that these special patterns of leadership only emerged for 3D interactions and not for 2D interactions. Results about the participants' experience of leadership, collaboration, embodiment, presence, and copresence shed further light on these findings.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PloS one 2017</div><img src='images/The_impact_of_self-avatars_on_trust_and_collaboration_in_shared_virtual_environments.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[The impact of self-avatars on trust and collaboration in shared virtual environments](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0189078)

**Ye Pan**, Anthony Steed

- A self-avatar is known to have a potentially significant impact on the user‚Äôs experience of the immersive content but it can also affect how users interact with each other in a shared virtual environment (SVE). We implemented an SVE for a consumer virtual reality system where each user‚Äôs body could be represented by a jointed self-avatar that was dynamically controlled by head and hand controllers. We investigated the impact of a self-avatar on collaborative outcomes such as completion time and trust formation during competitive and cooperative tasks. We used two different embodiment levels: no self-avatar and self-avatar, and compared these to an in-person face to face version of the tasks. We found that participants could finish the task more quickly when they cooperated than when they competed, for both the self-avatar condition and the face to face condition, but not for the no self-avatar condition. In terms of trust formation, both the self-avatar condition and the face to face condition led to higher scores than the no self-avatar condition; however, collaboration style had no significant effect on trust built between partners. The results are further evidence of the importance of a self-avatar representation in immersive virtual reality.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">VR 2016</div><img src='images/The_impact_of_a_self-avatar_on_cognitive_load_in_immersive_virtual_reality.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[The impact of a self-avatar on cognitive load in immersive virtual reality](https://ieeexplore.ieee.org/abstract/document/7504689)

Anthony Steed, **Ye Pan**, Fiona Zisch, William Steptoe

- The use of a self-avatar inside an immersive virtual reality system has been shown to have important effects on presence, interaction and perception of space. Based on studies from linguistics and cognition, in this paper we demonstrate that a self-avatar may aid the participant's cognitive processes while immersed in a virtual reality system. In our study participants were asked to memorise pairs of letters, perform a spatial rotation exercise and then recall the pairs of letters. In a between-subject factor they either had an avatar or not, and in a within-subject factor they were instructed to keep their hands still or not. We found that participants who both had an avatar and were allowed to move their hands had significantly higher letter pair recall. There was no significant difference between the other three conditions. Further analysis showed that participants who were allowed to move their hands, but could not see the self-avatar, usually didn't move their hands or stopped moving their hands after a short while. We argue that an active self-avatar may alleviate the mental load of doing the spatial rotation exercise and thus improve letter recall. The results are further evidence of the importance of an appropriate self-avatar representation in immersive virtual reality.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TVCG 2016</div><img src='images/An_in_the_wild_experiment_on_presence_and_embodiment_using_consumer_virtual_reality_equipment.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An ‚Äòin the wild‚Äôexperiment on presence and embodiment using consumer virtual reality equipment](https://ieeexplore.ieee.org/abstract/document/7383331)

Anthony Steed, Sebastian Frlston, Maria Murcia Lopez, Jason Drummond, **Ye Pan**, David Swapp

- Consumer virtual reality systems are now becoming widely available. We report on a study on presence and embodiment within virtual reality that was conducted `in the wild', in that data was collected from devices owned by consumers in uncontrolled settings, not in a traditional laboratory setting. Users of Samsung Gear VR and Google Cardboard devices were invited by web pages and email invitation to download and run an app that presented a scenario where the participant would sit in a bar watching a singer. Each participant saw one of eight variations of the scenario: with or without a self-avatar; singer inviting the participant to tap along or not; singer looking at the participant or not. Despite the uncontrolled situation of the experiment, results from an in-app questionnaire showed tentative evidence that a self-avatar had a positive effect on self-report of presence and embodiment, and that the singer inviting the participant to tap along had a negative effect on self-report of embodiment. We discuss the limitations of the study and the platforms, and the potential for future open virtual reality experiments.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Frontiers in Robotics and AI 2016</div><img src='images/A_comparison_of_avatar_video_and_robot_mediated_interaction_on_users_trust_in_expertise.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A comparison of avatar-, Video-, and robot-Mediated interaction on Users‚Äô Trust in expertise](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2016.00012/full)

**Ye Pan**, Anthony Steed

- Communication technologies are becoming increasingly diverse in form and functionality. A central concern is the ability to detect whether others are trustworthy. Judgments of trustworthiness rely, in part, on assessments of non-verbal cues, which are affected by media representations. In this research, we compared trust formation on three media representations. We presented 24 participants with advisors represented by two of the three alternate formats: video, avatar, or robot. Unknown to the participants, one was an expert, and the other was a non-expert. We observed participants‚Äô advice-seeking behavior under risk as an indicator of their trust in the advisor. We found that most participants preferred seeking advice from the expert, but we also found a tendency for seeking robot or video advice. Avatar advice, in contrast, was more rarely sought. Users‚Äô self-reports support these findings. These results suggest that when users make trust assessments, the physical presence of the robot representation might compensate for the lack of identity cues.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJHCS 2016</div><img src='images/Effects_of_3D_perspective_on_head_gaze_estimation_with_a_multiview_autostereoscopic_display.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Effects of 3D perspective on head gaze estimation with a multiview autostereoscopic display](https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.1638)

**Ye Pan**, Anthony Steed

- Head gaze, or the orientation of the head, is a very important attentional cue in face to face conversation. Some subtleties of the gaze can be lost in common teleconferencing systems, because a single perspective warps spatial characteristics. A recent random hole display is a potentially interesting display for group conversation, as it allows multiple stereo viewers in arbitrary locations, without the restriction of conventional autostereoscopic displays on viewing positions. We represented a remote person as an avatar on a random hole display. We evaluated this system by measuring the ability of multiple observers with different horizontal and vertical viewing angles to accurately and simultaneously judge which targets the avatar is gazing at. We compared three perspective conditions: a conventional 2D view, a monoscopic perspective-correct view, and a stereoscopic perspective-correct views. In the latter two conditions, the random hole display shows three and six views simultaneously. Although the random hole display does not provide high quality view, because it has to distribute display pixels among multiple viewers, the different views are easily distinguished. Results suggest the combined presence of perspective-correct and stereoscopic cues significantly improved the effectiveness with which observers were able to assess the avatar‚Äôs head gaze direction. This motivates the need for stereo in future multiview displays.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CAVW 2015</div><img src='images/Symmetric_telepresence_using_robotic_humanoid_surrogates.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Symmetric telepresence using robotic humanoid surrogates](https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.1638)

Arjun Nagendran, Anthony Steed, Brian Kelly, **Ye Pan**

- Telepresence involves the use of virtual reality technology to facilitate apparent physical participation in distant events, including potentially performing tasks, while creating a sense of being in that location. Traditionally, such systems are asymmetric in nature where only one side (participant) is ‚Äúteleported‚Äù to the remote location. In this manuscript, the authors explore the possibility of symmetric three-dimensional telepresence where both sides (participants) are ‚Äúteleported‚Äù simultaneously to each other's location; the overarching concept of symmetric telepresence in virtual environments is extended to telepresence robots in physical environments. Two identical physical humanoid robots located in UK and the USA serve as surrogates while performing a transcontinental shared collaborative task. The actions of these surrogate robots are driven by capturing the intent of the participants controlling them in either location. Participants could communicate verbally but could not see the other person or the remote location while performing the task. The effectiveness of gesturing along with other observations during this preliminary experiment is presented. Results reveal that the symmetric robotic telepresence allowed participants to use and understand gestures in cases where they would otherwise have to describe their actions verbally. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv preprint 2015</div><img src='images/An_empirical_study_on_display_ad_impression_viewability_measurements.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An empirical study on display ad impression viewability measurements](https://arxiv.org/abs/1505.05788)

Weinan Zhang, **Ye Pan**, Tianxiong Zhou, Jun Wang

- Display advertising normally charges advertisers for every single ad impression. Specifically, if an ad in a webpage has been loaded in the browser, an ad impression is counted. However, due to the position and size of the ad slot, lots of ads are actually not viewed but still measured as impressions and charged. These fraud ad impressions indeed undermine the efficacy of display advertising. A perfect ad impression viewability measurement should match what the user has really viewed with a short memory. In this paper, we conduct extensive investigations on display ad impression viewability measurements on dimensions of ad creative displayed pixel percentage and exposure time to find which measurement provides the most accurate ad impression counting. The empirical results show that the most accurate measurement counts one ad impression if more than 75% of the ad creative pixels have been exposed for at least 2 continuous seconds.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Presence 2015</div><img src='images/A_surround_video_capture_and_presentation_system_for_preservation_of_eye_gaze_in_teleconferencing_applications.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A surround video capture and presentation system for preservation of eye-gaze in teleconferencing applications](https://ieeexplore.ieee.org/abstract/document/7226422)

**Ye Pan**, Oyewole Oyekoya, Anthony Steed

- We propose a new video conferencing system that uses an array of cameras to capture a remote user and then show the video of that person on a spherical display. This telepresence system has two key advantages: (i) it can capture a near-correct image for any potential observer viewing direction because the cameras surround the user horizontally; and (ii) with view-dependent graphical representation on the spherical display, it is possible to tell where the remote user is looking from any viewpoint, whereas flat displays are visible only from the front. As a result, the display can more faithfully represent the gaze of the remote user. We evaluate this system by measuring the ability of observers to accurately judge which targets the actor is gazing at in two experiments. Results from the first experiment demonstrate the effectiveness of the camera array and spherical display system, in that it allows observers at multiple observing positions to accurately tell at which targets the remote user is looking. The second experiment further compared a spherical display with a planar display and provided detailed reasons for the improvement of our system in conveying gaze. We found two linear models for predicting the distortion introduced by misalignment of capturing cameras and the observer's viewing angles in video conferencing systems. Those models might be able to enable a correction for this distortion in future display configurations.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGCHI 2014</div><img src='images/A_gaze_preserving_situated_multiview_telepresence_system.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A gaze-preserving situated multiview telepresence system](https://dl.acm.org/doi/abs/10.1145/2556288.2557320)

**Ye Pan**, Anthony Steed

- Gaze, attention, and eye contact are important aspects of face to face communication, but some subtleties can be lost in videoconferencing because participants look at a single planar image of the remote user. We propose a low-cost cylindrical videoconferencing system that preserves gaze direction by providing perspective-correct images for multiple viewpoints around a conference table. We accomplish this by using an array of cameras to capture a remote person, and an array of projectors to present the camera images onto a cylindrical screen. The cylindrical screen reflects each image to a narrow viewing zone. The use of such a situated display allows participants to see the remote person from multiple viewing directions. We compare our system to three alternative display configurations. We demonstrate the effectiveness of our system by showing it allows multiple participants to simultaneously tell where the remote person is placing their gaze.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGCHI 2014</div><img src='images/Comparing_flat_and_spherical_displays_in_a_trust_scenario_in_avatar_mediated_interaction.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Comparing flat and spherical displays in a trust scenario in avatar-mediated interaction](https://dl.acm.org/doi/abs/10.1145/2556288.2557276)

**Ye Pan**, William Steptoe, Anthony Steed

- We report on two experiments that investigate the influence of display type and viewing angle on how people place their trust during avatar-mediated interaction. By monitoring advice seeking behavior, our first experiment demonstrates that if participants observe an avatar at an oblique viewing angle on a flat display, they are less able to discriminate between expert and non-expert advice than if they observe the avatar face-on. We then introduce a novel spherical display and a ray-traced rendering technique that can display an avatar that can be seen correctly from any viewing direction. We expect that a spherical display has advantages over a flat display because it better supports non-verbal cues, particularly gaze direction, since it presents a clear and undistorted viewing aspect at all angles. Our second experiment compares the spherical display to a flat display. Whilst participants can discriminate expert advice regardless of display, a negative bias towards the flat screen emerges at oblique viewing angles. This result emphasizes the ability of the spherical display to be viewed qualitatively similarly from all angles. Together the experiments demonstrate how trust can be altered depending on how one views the avatar.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">3DTV 2012</div><img src='images/Preserving_gaze_direction_in_teleconferencing_using_a_camera_array_and_a_spherical_display.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Preserving gaze direction in teleconferencing using a camera array and a spherical display](https://ieeexplore.ieee.org/abstract/document/6365433)

**Ye Pan**, Anthony Steed

- The movement of human gaze is very important in face to face conversation. Some of the quality of that movement is lost in videoconferencing because the participants look at a single planar image of the remote person. We use an array of cameras to capture a remote user, and then display video of that person on a spherical display. We compare the spherical display to a face to face setting and a planar display. We demonstrate the effectiveness of the camera array and spherical display system in that it allows observers to accurately judge where the remote user is placing their gaze. 
</div>
</div>


# üìñ Educations
- *2011.09 - 2015.02*, [University College London](https://www.ucl.ac.uk/), PhD.
- *2010.09 - 2011.09*, [University College London](https://www.ucl.ac.uk/), Master. 
- *2006.09 - 2010.07*, [University of Electronic Science and technology](https://www.uestc.edu.cn/), Bachelor. 

# üíª Work Experience
- *2019.11 - Present*, [Shanghai Jiao Tong University](https://en.sjtu.edu.cn/), Associate Professor.
- *2017.09 - 2019.10*, [University College London](https://www.ucl.ac.uk/), Associate Research Scientist.
- *2015.02 - 2017.09*, [University College London](https://www.ucl.ac.uk/), Postdoc.
